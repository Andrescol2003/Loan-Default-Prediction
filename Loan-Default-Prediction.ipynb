{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0911f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libarries \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, classification_report\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accesing to the respective Data Set\n",
    "\n",
    "df_input = pd.read_csv('input.csv')\n",
    "df_loan = pd.read_csv('loan_data.csv')\n",
    "df_output = pd.read_csv('output.csv')\n",
    "\n",
    "df_loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for nul values, data types ana NaN \n",
    "\n",
    "df_loan.isnull().sum()\n",
    "\n",
    "df_loan.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a56b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting our column on numeric \n",
    "\n",
    "df_loan = pd.get_dummies(df_loan, columns=['purpose'], drop_first=True)\n",
    "\n",
    "df_loan.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histo Plot \n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df_loan['fico'], bins=15, kde=True)\n",
    "plt.title(\"FICO Score Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='not.fully.paid', y='fico', data=df_loan)\n",
    "plt.title('FICO Score vs Loan Default')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ddd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='not.fully.paid', hue='purpose_credit_card', data=df_loan)\n",
    "plt.title(\"Loan Default by Purpose: Credit Card\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = df_loan.corr()\n",
    "\n",
    "# Set the figure size and create the heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Loan Data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b4165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking the amounrt of values that our target has \n",
    "df_loan['not.fully.paid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2930d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storagering the numeric data that need to be scalet before use XGBClassifier\n",
    "numeric_cols = ['int.rate', 'installment', 'log.annual.inc', 'dti',\n",
    "                'fico', 'days.with.cr.line', 'revol.bal', 'revol.util',\n",
    "                'inq.last.6mths'] #-----> this codel will continue after \n",
    "                                  #the stopbrake that the following celles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Scalar model \n",
    "scaler = StandardScaler() # Scalar Model\n",
    "\n",
    "df_loan[numeric_cols] = scaler.fit_transform(df_loan[numeric_cols]) # Apply scaler only to numeric columns\n",
    "print(f'Numeric Futers Scaler:\\n{df_loan[numeric_cols]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b071a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dif our target and futures\n",
    "X = df_loan.drop('not.fully.paid', axis=1)\n",
    "y = df_loan['not.fully.paid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd395239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting our data \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buildong our class weights to handle imbalance\n",
    "weights = compute_class_weight(class_weight='balanced',\n",
    "                               classes=np.unique(y_train),\n",
    "                               y=y_train)\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weight_dict = {0: weights[0], 1: weights[1]}\n",
    "\n",
    "\n",
    "print(f\"Computed class weights: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27507824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Building the architecture of the Neural Network\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer + first hidden layer\n",
    "model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Therd \n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output Layer \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile Model \n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['Recall'])  # Track recall directly\n",
    "\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d98db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiting our neural network\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    class_weight=class_weight_dict,\n",
    "                    validation_data=(X_test, y_test),\\\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1e5a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted probabilities for the test set, flatten the array,\n",
    "# then apply a threshold of 0.4 to convert probabilities to binary class predictions (0 or 1)\n",
    "y_probs = model.predict(X_test).ravel()\n",
    "y_pred = (y_probs>= 0.4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f4bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Confussin Matrix:\\n', confusion_matrix(y_test,y_pred))\n",
    "print(f'Classification Report:\\n',classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through different class weight values for the minority class to test their effect on model performance.\n",
    "# For each weight, update the class_weight dictionary, retrain the model, and print the classification report.\n",
    "\n",
    "for weight in [4, 5, 6]:\n",
    "    print(f\"\\nTesting with class_weight = {{0:1, 1:{weight}}}\")\n",
    "\n",
    "    class_weight_dict = {0:1, 1:weight}\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "              epochs=30,\n",
    "              batch_size=32,\n",
    "              class_weight=class_weight_dict,\n",
    "              verbose=0) \n",
    "\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d027a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different probability thresholds to see how they affect classification performance.\n",
    "# For each threshold, convert predicted probabilities to binary predictions and print the classification report.\n",
    "\n",
    "thresholds = [0.35, 0.4, 0.45]\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred = (y_probs >= thresh).astype(int)\n",
    "    print(f\"\\nTesting with threshold = {thresh}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(f'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c317e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up early stopping to halt training when the validation recall stops improving for 5 epochs,\n",
    "# and restore the best model weights observed during training.\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_record', patience=5, restore_best_weights=True,mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with early stopping, using class weights to handle imbalance,\n",
    "# and validate on the test set after each epoch. Training will stop early if validation recall does not improve for 5 epochs.\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=15,\n",
    "                    batch_size=32,\n",
    "                    class_weight=class_weight_dict,\n",
    "                    validation_data=(X_test, y_test),\\\n",
    "                    callbacks=[earlystop]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b7cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to get the prediction back afetr adding early stop tou our {history}\n",
    "y_probs = model.predict(X_test).ravel()\n",
    "y_pred = (y_probs >= 0.4).astype(int)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2168acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds to test\n",
    "thresholds = [0.45, 0.5, 0.55]\n",
    "\n",
    "# Get predicted probabilities from the last trained model\n",
    "y_probs = model.predict(X_test).ravel()\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred = (y_probs >= thresh).astype(int)\n",
    "    print(f\"\\n Testing with threshold = {thresh}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e2cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_ratio = sum(y_train == 0)/sum(y_train == 1)\n",
    "print(f'Imbalance Ratio (scale_pos_weight):\\n {imbalance_ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04636f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the XGBoost classifier with parameters to handle class imbalance and set evaluation metric.\n",
    "\n",
    "model = XGBClassifier(\n",
    "    scale_per_weight = imbalance_ratio,\n",
    "    use_label_incoder = False, \n",
    "    eval_metric = 'logloss',\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiting model \n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the predictions \n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid for XGBoost to search over during grid search.\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'n_stimqtors': [100 , 200],\n",
    "    'subsample': [0.8, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost classifier with class imbalance handling and specific evaluation metric,\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    scale_pos_weight=imbalance_ratio,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up grid search to find the best hyperparameters for the XGBoost model.\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"\\nBest parameters: {grid.best_params_}\")\n",
    "\n",
    "# Best model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the classes in the training data by oversampling the minority class.\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Before SMOTE: {np.bincount(y_train)}\")\n",
    "print(f\"After SMOTE: {np.bincount(y_train_res)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c769a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41c81f",
   "metadata": {},
   "source": [
    "# Project Summary Notes\n",
    "\n",
    "In this project, we aimed to predict loan defaults using deep learning and classical machine learning models.\n",
    "\n",
    "The main challenge was the strong class imbalance, with around 80% non-defaults and 20% defaults.\n",
    "\n",
    "To address this, we tested:\n",
    "\n",
    "- Neural networks (Sequential, Keras) with:\n",
    "  - Class weights\n",
    "  - Dropout and L2 regularization\n",
    "  - Early stopping\n",
    "  - Threshold tuning\n",
    "\n",
    "- XGBoost and RandomForest with:\n",
    "  - `scale_pos_weight` parameter\n",
    "  - Manual hyperparameter tuning\n",
    "  - GridSearchCV for automated tuning\n",
    "\n",
    "- Synthetic oversampling using SMOTE\n",
    "\n",
    "- Ensemble methods (EnsembleVoteClassifier) combining XGBoost, RandomForest, and LogisticRegression\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- Neural networks overfit easily, even with regularization.\n",
    "- Tree-based models and ensembles improved robustness but still struggled to lift recall on the minority class.\n",
    "- The extreme class imbalance remained the key limitation affecting performance.\n",
    "\n",
    "**Future suggestions:**\n",
    "\n",
    "- Explore more advanced imbalance strategies like ADASYN or cost-sensitive learning.\n",
    "- Consider feature engineering or domain-specific insights to improve model separation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andres_env_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
